"""
Dataset Exporter for various formats.

Supports JSONL, Hugging Face Datasets, Alpaca, and ShareGPT formats.
"""

from __future__ import annotations

import json
import os
from datetime import datetime
from pathlib import Path
from typing import Any

from datasets import Dataset


class DatasetExporter:
    """Export generated data to various formats.

    Supported formats:
    - JSONL: One JSON object per line
    - Hugging Face Dataset: Native HF format with optional Hub upload
    - Alpaca: Stanford Alpaca training format
    - ShareGPT: Conversation format for chat models
    """

    def __init__(self, output_dir: str = "/app/outputs") -> None:
        """Initialize the exporter.

        Args:
            output_dir: Directory to save exported files
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def export(
        self,
        data: list[dict[str, Any]],
        format_type: str,
        name: str,
        hf_repo: str | None = None,
        private: bool = False,
    ) -> str:
        """
        Export data to the specified format.
        
        Args:
            data: List of instruction-response pairs
            format_type: Export format (jsonl, hugging_face_dataset, alpaca_format, sharegpt_format)
            name: Output filename or dataset name
            hf_repo: Hugging Face repository ID for upload
            private: Whether HF repo should be private
            
        Returns:
            Path to exported file or HF repo URL
        """
        
        if format_type == "jsonl":
            return self._export_jsonl(data, name)
        elif format_type == "hugging_face_dataset":
            return self._export_hf_dataset(data, name, hf_repo, private)
        elif format_type == "alpaca_format":
            return self._export_alpaca(data, name)
        elif format_type == "sharegpt_format":
            return self._export_sharegpt(data, name)
        else:
            raise ValueError(f"Unknown format: {format_type}")
    
    def _export_jsonl(self, data: list[dict[str, Any]], name: str) -> str:
        """Export as JSONL (one JSON object per line)."""
        
        filepath = self.output_dir / f"{name}.jsonl"
        
        with open(filepath, "w", encoding="utf-8") as f:
            for item in data:
                # Clean up metadata, keep only instruction and response
                clean_item = {
                    "instruction": item.get("instruction", ""),
                    "response": item.get("response", ""),
                }
                # Optionally include metadata
                if "input" in item:
                    clean_item["input"] = item["input"]
                
                f.write(json.dumps(clean_item, ensure_ascii=False) + "\n")
        
        return str(filepath)
    
    def _export_hf_dataset(
        self,
        data: list[dict[str, Any]],
        name: str,
        hf_repo: str | None = None,
        private: bool = False,
    ) -> str:
        """Export as Hugging Face Dataset and optionally upload."""
        
        # Convert to HF Dataset
        dataset_dict = {
            "instruction": [item.get("instruction", "") for item in data],
            "response": [item.get("response", "") for item in data],
        }
        
        # Add optional fields if present
        if any("input" in item for item in data):
            dataset_dict["input"] = [item.get("input", "") for item in data]
        
        if any("method" in item for item in data):
            dataset_dict["method"] = [item.get("method", "") for item in data]
        
        dataset = Dataset.from_dict(dataset_dict)
        
        # Save locally
        local_path = self.output_dir / name
        dataset.save_to_disk(str(local_path))
        
        # Upload to HF if repo specified
        if hf_repo:
            from huggingface_hub import HfApi
            
            hf_token = os.getenv("HF_TOKEN")
            if not hf_token:
                raise ValueError("HF_TOKEN environment variable not set")
            
            dataset.push_to_hub(
                hf_repo,
                token=hf_token,
                private=private,
                commit_message=f"Upload dataset generated by Seedling on {datetime.now().isoformat()}"
            )
            
            return f"https://huggingface.co/datasets/{hf_repo}"
        
        return str(local_path)
    
    def _export_alpaca(self, data: list[dict[str, Any]], name: str) -> str:
        """
        Export in Alpaca format.
        Format: {"instruction": ..., "input": ..., "output": ...}
        """
        
        filepath = self.output_dir / f"{name}_alpaca.json"
        
        alpaca_data = []
        for item in data:
            alpaca_item = {
                "instruction": item.get("instruction", ""),
                "input": item.get("input", ""),  # Optional context
                "output": item.get("response", ""),
            }
            alpaca_data.append(alpaca_item)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(alpaca_data, f, ensure_ascii=False, indent=2)
        
        return str(filepath)
    
    def _export_sharegpt(self, data: list[dict[str, Any]], name: str) -> str:
        """
        Export in ShareGPT/OpenAI format.
        Format: {"conversations": [{"from": "human", "value": ...}, {"from": "gpt", "value": ...}]}
        """
        
        filepath = self.output_dir / f"{name}_sharegpt.json"
        
        sharegpt_data = []
        for item in data:
            conversation = {
                "conversations": [
                    {
                        "from": "human",
                        "value": item.get("instruction", "")
                    },
                    {
                        "from": "gpt", 
                        "value": item.get("response", "")
                    }
                ]
            }
            
            # Add system prompt if available
            if "system_prompt" in item:
                conversation["conversations"].insert(0, {
                    "from": "system",
                    "value": item["system_prompt"]
                })
            
            sharegpt_data.append(conversation)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(sharegpt_data, f, ensure_ascii=False, indent=2)
        
        return str(filepath)


class ArgillaExporter:
    """Export data to Argilla for curation.

    Supports both legacy Argilla 1.x API and modern Argilla 2.x SDK.
    """

    def __init__(
        self,
        api_url: str = "http://localhost:6900",
        api_key: str = "argilla.apikey",
    ) -> None:
        """Initialize the Argilla exporter.

        Args:
            api_url: URL of the Argilla server
            api_key: API key for authentication
        """
        self.api_url = api_url
        self.api_key = api_key

    def push_to_argilla(
        self,
        data: list[dict[str, Any]],
        dataset_name: str,
        workspace: str = "admin",
    ) -> str:
        """Push data to Argilla for review and curation.

        Args:
            data: List of instruction-response pairs
            dataset_name: Name for the Argilla dataset
            workspace: Argilla workspace name

        Returns:
            URL to the created dataset
        """
        import argilla as rg

        # Initialize client (Argilla 2.x style)
        client = rg.Argilla(
            api_url=self.api_url,
            api_key=self.api_key,
        )

        # Define the dataset settings for text generation review
        settings = rg.Settings(
            fields=[
                rg.TextField(name="instruction", title="Instruction"),
                rg.TextField(name="response", title="Response"),
            ],
            questions=[
                rg.RatingQuestion(
                    name="quality",
                    title="Quality Rating",
                    description="Rate the quality of the instruction-response pair",
                    values=[1, 2, 3, 4, 5],
                ),
                rg.TextQuestion(
                    name="feedback",
                    title="Feedback",
                    description="Optional feedback or corrections",
                    required=False,
                ),
                rg.LabelQuestion(
                    name="status",
                    title="Status",
                    labels=["approved", "rejected", "needs_revision"],
                ),
            ],
            metadata=[
                rg.TermsMetadataProperty(name="method", title="Generation Method"),
                rg.TermsMetadataProperty(name="model", title="Model"),
            ],
        )

        # Create or get dataset
        try:
            dataset = rg.Dataset(
                name=dataset_name,
                workspace=workspace,
                settings=settings,
                client=client,
            )
            dataset.create()
        except Exception:
            # Dataset might already exist
            dataset = client.datasets(name=dataset_name, workspace=workspace)

        # Create records
        records = [
            rg.Record(
                fields={
                    "instruction": item.get("instruction", ""),
                    "response": item.get("response", ""),
                },
                metadata={
                    "method": item.get("method", "unknown"),
                    "model": item.get("model", "unknown"),
                },
            )
            for item in data
        ]

        # Log records to dataset
        dataset.records.log(records)

        return f"{self.api_url}/dataset/{dataset.id}/annotation-mode"
