"""
Dataset Exporter for various formats
Supports JSONL, Hugging Face Datasets, Alpaca, and ShareGPT formats.
"""

import json
import os
from pathlib import Path
from datetime import datetime
from typing import Optional

from datasets import Dataset


class DatasetExporter:
    """Export generated data to various formats."""
    
    def __init__(self, output_dir: str = "/app/outputs"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
    
    def export(
        self,
        data: list[dict],
        format_type: str,
        name: str,
        hf_repo: Optional[str] = None,
        private: bool = False
    ) -> str:
        """
        Export data to the specified format.
        
        Args:
            data: List of instruction-response pairs
            format_type: Export format (jsonl, hugging_face_dataset, alpaca_format, sharegpt_format)
            name: Output filename or dataset name
            hf_repo: Hugging Face repository ID for upload
            private: Whether HF repo should be private
            
        Returns:
            Path to exported file or HF repo URL
        """
        
        if format_type == "jsonl":
            return self._export_jsonl(data, name)
        elif format_type == "hugging_face_dataset":
            return self._export_hf_dataset(data, name, hf_repo, private)
        elif format_type == "alpaca_format":
            return self._export_alpaca(data, name)
        elif format_type == "sharegpt_format":
            return self._export_sharegpt(data, name)
        else:
            raise ValueError(f"Unknown format: {format_type}")
    
    def _export_jsonl(self, data: list[dict], name: str) -> str:
        """Export as JSONL (one JSON object per line)."""
        
        filepath = self.output_dir / f"{name}.jsonl"
        
        with open(filepath, "w", encoding="utf-8") as f:
            for item in data:
                # Clean up metadata, keep only instruction and response
                clean_item = {
                    "instruction": item.get("instruction", ""),
                    "response": item.get("response", ""),
                }
                # Optionally include metadata
                if "input" in item:
                    clean_item["input"] = item["input"]
                
                f.write(json.dumps(clean_item, ensure_ascii=False) + "\n")
        
        return str(filepath)
    
    def _export_hf_dataset(
        self,
        data: list[dict],
        name: str,
        hf_repo: Optional[str] = None,
        private: bool = False
    ) -> str:
        """Export as Hugging Face Dataset and optionally upload."""
        
        # Convert to HF Dataset
        dataset_dict = {
            "instruction": [item.get("instruction", "") for item in data],
            "response": [item.get("response", "") for item in data],
        }
        
        # Add optional fields if present
        if any("input" in item for item in data):
            dataset_dict["input"] = [item.get("input", "") for item in data]
        
        if any("method" in item for item in data):
            dataset_dict["method"] = [item.get("method", "") for item in data]
        
        dataset = Dataset.from_dict(dataset_dict)
        
        # Save locally
        local_path = self.output_dir / name
        dataset.save_to_disk(str(local_path))
        
        # Upload to HF if repo specified
        if hf_repo:
            from huggingface_hub import HfApi
            
            hf_token = os.getenv("HF_TOKEN")
            if not hf_token:
                raise ValueError("HF_TOKEN environment variable not set")
            
            dataset.push_to_hub(
                hf_repo,
                token=hf_token,
                private=private,
                commit_message=f"Upload dataset generated by Seedling on {datetime.now().isoformat()}"
            )
            
            return f"https://huggingface.co/datasets/{hf_repo}"
        
        return str(local_path)
    
    def _export_alpaca(self, data: list[dict], name: str) -> str:
        """
        Export in Alpaca format.
        Format: {"instruction": ..., "input": ..., "output": ...}
        """
        
        filepath = self.output_dir / f"{name}_alpaca.json"
        
        alpaca_data = []
        for item in data:
            alpaca_item = {
                "instruction": item.get("instruction", ""),
                "input": item.get("input", ""),  # Optional context
                "output": item.get("response", ""),
            }
            alpaca_data.append(alpaca_item)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(alpaca_data, f, ensure_ascii=False, indent=2)
        
        return str(filepath)
    
    def _export_sharegpt(self, data: list[dict], name: str) -> str:
        """
        Export in ShareGPT/OpenAI format.
        Format: {"conversations": [{"from": "human", "value": ...}, {"from": "gpt", "value": ...}]}
        """
        
        filepath = self.output_dir / f"{name}_sharegpt.json"
        
        sharegpt_data = []
        for item in data:
            conversation = {
                "conversations": [
                    {
                        "from": "human",
                        "value": item.get("instruction", "")
                    },
                    {
                        "from": "gpt", 
                        "value": item.get("response", "")
                    }
                ]
            }
            
            # Add system prompt if available
            if "system_prompt" in item:
                conversation["conversations"].insert(0, {
                    "from": "system",
                    "value": item["system_prompt"]
                })
            
            sharegpt_data.append(conversation)
        
        with open(filepath, "w", encoding="utf-8") as f:
            json.dump(sharegpt_data, f, ensure_ascii=False, indent=2)
        
        return str(filepath)


class ArgillaExporter:
    """Export data to Argilla for curation."""
    
    def __init__(
        self,
        api_url: str = "http://localhost:6900",
        api_key: str = "argilla.apikey"
    ):
        self.api_url = api_url
        self.api_key = api_key
    
    def push_to_argilla(
        self,
        data: list[dict],
        dataset_name: str,
        workspace: str = "admin"
    ) -> str:
        """Push data to Argilla for review and curation."""
        
        import argilla as rg
        
        # Initialize client
        rg.init(
            api_url=self.api_url,
            api_key=self.api_key
        )
        
        # Create records
        records = []
        for item in data:
            record = rg.TextClassificationRecord(
                text=f"**Instruction:**\n{item.get('instruction', '')}\n\n**Response:**\n{item.get('response', '')}",
                metadata={
                    "instruction": item.get("instruction", ""),
                    "response": item.get("response", ""),
                    "method": item.get("method", "unknown"),
                    "model": item.get("model", "unknown"),
                }
            )
            records.append(record)
        
        # Log to Argilla
        rg.log(
            records=records,
            name=dataset_name,
            workspace=workspace
        )
        
        return f"{self.api_url}/datasets/{workspace}/{dataset_name}"
